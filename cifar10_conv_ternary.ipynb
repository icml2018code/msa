{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_cifar10_data\n",
    "x_train, y_train, x_validate, y_validate, x_test, y_test = load_cifar10_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network, train, utils\n",
    "from layers import ReluLayer, TernaryFullyConnectedLayer, \\\n",
    "    TernaryConvolutionLayer, BatchNormLayer, MaxPoolingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = network.NeuralNetwork(in_size=[None, 32, 32, 3], n_out_classes=10, \n",
    "                           loss_func=utils.smooth_hinge_loss)\n",
    "nn.reset_graph()\n",
    "\n",
    "# Hidden Conv-1\n",
    "nn.add_layer(TernaryConvolutionLayer(\n",
    "    out_dim=128, filter_size=3))\n",
    "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Conv-2\n",
    "nn.add_layer(TernaryConvolutionLayer(\n",
    "    out_dim=128, filter_size=3))\n",
    "nn.add_layer(MaxPoolingLayer())\n",
    "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Conv-3\n",
    "nn.add_layer(TernaryConvolutionLayer(\n",
    "    out_dim=256, filter_size=3))\n",
    "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Conv-4\n",
    "nn.add_layer(TernaryConvolutionLayer(\n",
    "    out_dim=256, filter_size=3))\n",
    "nn.add_layer(MaxPoolingLayer())\n",
    "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Conv-5\n",
    "nn.add_layer(TernaryConvolutionLayer(\n",
    "    out_dim=512, filter_size=3))\n",
    "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Conv-6\n",
    "nn.add_layer(TernaryConvolutionLayer(\n",
    "    out_dim=512, filter_size=3))\n",
    "nn.add_layer(MaxPoolingLayer())\n",
    "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Fc-7\n",
    "nn.add_layer(TernaryFullyConnectedLayer(\n",
    "    out_dim=1024))\n",
    "nn.add_layer(BatchNormLayer(axes=[0]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Fc-8\n",
    "nn.add_layer(TernaryFullyConnectedLayer(\n",
    "    out_dim=1024))\n",
    "nn.add_layer(BatchNormLayer(axes=[0]))\n",
    "nn.add_layer(ReluLayer())\n",
    "\n",
    "# Hidden Fc-9\n",
    "nn.add_layer(TernaryFullyConnectedLayer(out_dim=10))\n",
    "nn.add_layer(BatchNormLayer(axes=[0]))\n",
    "\n",
    "nn.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = (x_train, y_train)\n",
    "opt = train.Trainer(nn, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "opt.set_rho(0.25)\n",
    "opt.set_ema_rates(0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_and_accs_train = []\n",
    "losses_and_accs_valid = []\n",
    "losses_and_accs_test = []\n",
    "\n",
    "sparsity_fracs = []\n",
    "\n",
    "n_epochs = 500\n",
    "\n",
    "for t in range(n_epochs):    \n",
    "    print('Epoch: ', t)\n",
    "\n",
    "    opt.train_epoch(batch_size=100, ema_decay=0.98, n_output=10, verbose=True)\n",
    "\n",
    "\n",
    "    losses_and_accs_train.append(\n",
    "        opt.loss_and_accuracy((x_train, y_train), max_batch=400, inference=True))\n",
    "    losses_and_accs_test.append(\n",
    "        opt.loss_and_accuracy((x_test, y_test), max_batch=400, inference=True))\n",
    "    losses_and_accs_valid.append(\n",
    "        opt.loss_and_accuracy((x_validate, y_validate), max_batch=400, inference=True))\n",
    "    \n",
    "    sparsity_fracs.append(utils.get_sparsity_frac(nn, opt))\n",
    "    \n",
    "    print('Train loss/acc: ', losses_and_accs_train[-1],\n",
    "          'Test loss/acc: ', losses_and_accs_test[-1])\n",
    "    \n",
    "losses_and_accs_train = np.asarray(losses_and_accs_train)\n",
    "losses_and_accs_valid = np.asarray(losses_and_accs_valid)\n",
    "losses_and_accs_test = np.asarray(losses_and_accs_test)\n",
    "sparsity_fracs = np.asarray(sparsity_fracs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: ', opt.loss_and_accuracy((x_train, y_train), inference=True,\n",
    "                                       max_batch=400))\n",
    "print('Valid: ', opt.loss_and_accuracy((x_validate, y_validate), inference=True,\n",
    "                                      max_batch=400))\n",
    "print('Test: ', opt.loss_and_accuracy((x_test, y_test), inference=True,\n",
    "                                     max_batch=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = np.argmax(losses_and_accs_valid[:,1]) + 1\n",
    "print('Best epoch: ', best_epoch)\n",
    "print('Train acc: ', losses_and_accs_train[best_epoch-1, 1])\n",
    "print('Valid acc: ', losses_and_accs_valid[best_epoch-1, 1])\n",
    "print('Test acc: ', losses_and_accs_test[best_epoch-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "losses_and_accs = np.concatenate(\n",
    "    [np.asarray(losses_and_accs_train),\n",
    "     np.asarray(losses_and_accs_valid),\n",
    "     np.asarray(losses_and_accs_test)], axis=1)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\n",
    "ax1.semilogy(losses_and_accs[:,0], '-o', label='Train loss')\n",
    "ax1.semilogy(losses_and_accs[:,2], '-o', label='Valid loss')\n",
    "ax1.semilogy(losses_and_accs[:,4], '-o', label='Test loss')\n",
    "\n",
    "ax2.plot(losses_and_accs[:,1], '-o', label='Train acc')\n",
    "ax2.plot(losses_and_accs[:,3], '-o', label='Valid acc')\n",
    "ax2.plot(losses_and_accs[:,5], '-o', label='Test acc')\n",
    "\n",
    "for ax in [ax1,ax2]:\n",
    "    ax.legend()\n",
    "\n",
    "ax2.set_ylim(0.1,1)\n",
    "    \n",
    "print('Final results: ', losses_and_accs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sparsity fracs\n",
    "plt.plot(sparsity_fracs)\n",
    "print('Final sparsity fraction: ', sparsity_fracs[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
